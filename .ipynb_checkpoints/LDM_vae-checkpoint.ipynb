{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57f71ec-e269-4510-a91d-c2e4a03ae74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from dataset.semKITTI_dataset import SemKITTI_DVPS_Dataset\n",
    "from diffusers import AutoencoderKL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from ldmseg.models import GeneralVAESeg\n",
    "from ldmseg.trainers import TrainerAE\n",
    "from ldmseg.utils import prepare_config, Logger, is_main_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f12638-4fd6-45ef-b9a8-0464cd5b7537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_classifier(model, dataloader, num_epochs=20, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    针对 KITTI 像素级分类训练：\n",
    "      - 模型输出 logits 形状应为 [B, 19, H, W]\n",
    "      - Ground Truth segments 形状为 [B, H, W]，取值范围 0～18\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # 假设 dataloader 返回 (images, depths, segments, instances)\n",
    "            images, depths, segments, instances = batch[0],batch[1],batch[2],batch[3]\n",
    "            images = images.to(device)\n",
    "            segments = segments.to(device)  # segments 应为 [B, H, W] 且类型为 long\n",
    "            if segments.ndim == 4 and segments.shape[1] == 1:\n",
    "                segments = segments.squeeze(1).long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播：输出 sample 部分应为 logits，形状 [B, 19, h, w]\n",
    "            output = model(images, sample_posterior=True)\n",
    "            logits = output.sample\n",
    "            # 如果输出尺寸与目标尺寸不一致，通过双线性上采样调整到 (192, 640)\n",
    "            logits = F.interpolate(logits, size=(192, 640), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            loss = criterion(logits, segments)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / (i+1):.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51392a39-1997-47c7-a874-02cf7611ad21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation factor:  2\n",
      "Parametrization:  gaussian\n",
      "Activation function:  none\n",
      "GeneralVAESeg(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): SiLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): SiLU()\n",
      "    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (7): SiLU()\n",
      "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (10): SiLU()\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): Identity()\n",
      "    (13): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "    (14): SiLU()\n",
      "    (15): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Conv2d(4, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (3): LayerNorm2d()\n",
      "    (4): SiLU()\n",
      "    (5): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (6): LayerNorm2d()\n",
      "    (7): SiLU()\n",
      "    (8): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    (9): SiLU()\n",
      "    (10): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((192, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 定义分割标签（Ground Truth）预处理\n",
    "# 使用 PILToTensor 保持标签原始值，再通过 Lambda 转换为 [H, W] long 张量\n",
    "GT_transforms = transforms.Compose([\n",
    "    transforms.Resize((192, 640), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.ToTensor(),  # 输出形状为 [C, H, W]，类型为 uint8\n",
    "])\n",
    "\n",
    "# 数据集根目录（请根据你的路径修改）\n",
    "dataset_root = '/root/autodl-tmp/video_sequence'\n",
    "\n",
    "# 构造训练集\n",
    "train_dataset = SemKITTI_DVPS_Dataset(\n",
    "    root=dataset_root,\n",
    "    split='train',\n",
    "    image_transform=image_transforms,\n",
    "    GT_transform=GT_transforms\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=False, num_workers=4)\n",
    "\n",
    "# -----------------------\n",
    "# 实例化模型\n",
    "# 对于分类任务，将 out_channels 设置为类别数 19\n",
    "vae = GeneralVAESeg(\n",
    "    in_channels= 7,  # consider bit encoding\n",
    "   int_channels= 256,\n",
    "   out_channels= 128,\n",
    "   block_out_channels= [32, 64, 128, 256],\n",
    "   latent_channels= 4,\n",
    "   num_latents= 2,\n",
    "   num_upscalers= 2,\n",
    "   upscale_channels= 256,\n",
    "   norm_num_groups= 32,\n",
    "   scaling_factor= 0.2,\n",
    "   parametrization= 'gaussian',\n",
    "   act_fn= 'none',\n",
    "   clamp_output= False,\n",
    "   freeze_codebook= False,\n",
    "   num_mid_blocks= 0,\n",
    "   fuse_rgb= False,\n",
    "   resize_input= False,\n",
    "   skip_encoder= False,\n",
    ")\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f442b94-cab7-4b53-a89e-32486b06e09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): SiLU()\n",
      "  (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (4): SiLU()\n",
      "  (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (7): SiLU()\n",
      "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (10): SiLU()\n",
      "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (12): Identity()\n",
      "  (13): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "  (14): SiLU()\n",
      "  (15): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vae.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b62ac3a2-a5b7-4548-b8d1-30fc825bfbc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    \"\"\"\n",
    "    计算 Dice 损失（用于衡量区域重叠）。\n",
    "    pred: 模型输出 logits，形状 [B, C, H, W]，需先 softmax\n",
    "    target: ground truth，形状 [B, H, W]（整数标签）\n",
    "    \"\"\"\n",
    "    pred_soft = F.softmax(pred, dim=1)\n",
    "    target_one_hot = F.one_hot(target, num_classes=pred.shape[1]).permute(0,3,1,2).float()\n",
    "    intersection = (pred_soft * target_one_hot).sum(dim=(2,3))\n",
    "    union = pred_soft.sum(dim=(2,3)) + target_one_hot.sum(dim=(2,3))\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - dice.mean()\n",
    "\n",
    "def panoptic_loss(sem_logits, inst_logits, sem_target, inst_target, lambda_dice=1.0):\n",
    "    \"\"\"\n",
    "    对两个输出 head 分别计算交叉熵损失和可选的 Dice 损失，并组合成总损失。\n",
    "    \n",
    "    sem_logits: [B, num_sem_classes, H, W] 语义分支输出\n",
    "    inst_logits: [B, num_inst, H, W] 实例分支输出\n",
    "    sem_target: [B, H, W] ground truth 语义标签\n",
    "    inst_target: [B, H, W] ground truth 实例标签\n",
    "    lambda_dice: Dice 损失权重\n",
    "    \"\"\"\n",
    "    sem_ce = F.cross_entropy(sem_logits, sem_target)\n",
    "    inst_ce = F.cross_entropy(inst_logits, inst_target)\n",
    "    sem_dice = dice_loss(sem_logits, sem_target)\n",
    "    inst_dice = dice_loss(inst_logits, inst_target)\n",
    "    total_loss = sem_ce + inst_ce + lambda_dice * (sem_dice + inst_dice)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56f846-fb03-4a32-83da-ca6a4efcf5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDMSeg",
   "language": "python",
   "name": "ldmseg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
